{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK : READ, TRANSFORM & PROCESS @ SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students' Names : Sarah Gutierez & Adrien Houpert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pyspark official documentation\n",
    "\n",
    "Based on the slides of the course and the spark documentation available online, for each cell, write the spark code that will answer the instructions.\n",
    "\n",
    "You have to follow the tutorials and search over internet in order to get hand's on experience with pyspark, the notebook gives you some suggestions but you are free to try new things and explore the possibilities offered pyspark possibilities.\n",
    "\n",
    "List of tutorial to follow :\n",
    "\n",
    "    - The pyspark API quickstart : https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n",
    "        Please be sure to read the DataFrame API before going further.\n",
    "    - The Spark SQL guide : https://spark.apache.org/docs/latest/sql-getting-started.html\n",
    "        This is the most complete guide on how to use Spark SQL. Be sur to do the getting started and the data sources part\n",
    "\n",
    "Important : Create cells to demonstrate the code you write.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/lib/python3.10/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /usr/lib/python3.10/site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  pyspark basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Application_Name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/12 16:47:54 WARN Utils: Your hostname, Unikarah resolves to a loopback address: 127.0.1.1; using 192.168.1.96 instead (on interface wlo1)\n",
      "22/05/12 16:47:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/12 16:47:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the createDataFrame function, create a spark dataframe with the values [(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)] and the columns [\"name\", \"age\"], give this dataframe the name data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),(\"TD\", 35), (\"Brooke\", 25)], [\"name\", \"age\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will show the schema of data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will display 3 values of the dataframe data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(name='Brooke', age=20),\n",
       " Row(name='Denny', age=31),\n",
       " Row(name='Jules', age=30)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will return a dataframe with 5 values of the data_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Brooke', age=20),\n",
       " Row(name='Denny', age=31),\n",
       " Row(name='Jules', age=30),\n",
       " Row(name='TD', age=35),\n",
       " Row(name='Brooke', age=25)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_5 = data_df.head(5)\n",
    "data_df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark gives the possibility to read a wide range of data sources by natively supporting the read of a huge number of file types and databases\n",
    "\n",
    "In the following, write the line of code that will allow you to read the json file specified in the json_file_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"data/devices.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/unikarah/Documents/ING2/BigData\n"
     ]
    }
   ],
   "source": [
    "spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will allow you to read a csv file specified in the csv_file_path variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"data/devices.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function 1 : read_disp_info()\n",
    "Write a function, named read_disp_info, which takes as input a file path (json or csv path) and give the following information : It creates a dataframe from the file path, display it's schema, display the 10 first line of the dataframe and give the total number of rows of the dataframe\n",
    "It is important to understand that this function should be able to read the json and csv files, to do this, it can look at the end of filepath string in order to deduce if the extension of the file is csv or json and then call the reading function accordinly to the extension. At the end, this function return a spark dataframe object representing the read data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_disp_info(file):\n",
    "    _, extension = os.path.splitext(file)\n",
    "    if extension == '.csv':\n",
    "        df = spark.read.csv(file)\n",
    "    elif extension == '.json':\n",
    "        df = spark.read.json(file)\n",
    "    else:\n",
    "        raise Exception('Wrong file: csv or json needed')\n",
    "    df.printSchema()\n",
    "    print(df.head(10))\n",
    "    print(f\"\\nNumber of rows {df.count()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, read the file present in \"/home/adminefrei/Documents/data/devices.json\" and affect the retuned dataframe to the variable devDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dev_type: string (nullable = true)\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: string (nullable = true)\n",
      "\n",
      "[Row(dev_type='phone', devnum=1, make='Sorrento', model='F00L', release_dt='2008-10-21T00:00:00.000-07:00'), Row(dev_type='phone', devnum=2, make='Titanic', model='2100', release_dt='2010-04-19T00:00:00.000-07:00'), Row(dev_type='phone', devnum=3, make='MeeToo', model='3.0', release_dt='2011-02-18T00:00:00.000-08:00'), Row(dev_type='phone', devnum=4, make='MeeToo', model='3.1', release_dt='2011-09-21T00:00:00.000-07:00'), Row(dev_type='phone', devnum=5, make='iFruit', model='1', release_dt='2008-10-21T00:00:00.000-07:00'), Row(dev_type='phone', devnum=6, make='iFruit', model='3', release_dt='2011-11-02T00:00:00.000-07:00'), Row(dev_type='phone', devnum=7, make='iFruit', model='2', release_dt='2010-05-20T00:00:00.000-07:00'), Row(dev_type='phone', devnum=8, make='iFruit', model='5', release_dt='2013-07-02T00:00:00.000-07:00'), Row(dev_type='phone', devnum=9, make='Titanic', model='1000', release_dt='2008-10-21T00:00:00.000-07:00'), Row(dev_type='phone', devnum=10, make='MeeToo', model='1.0', release_dt='2008-10-21T00:00:00.000-07:00')]\n",
      "\n",
      "Number of rows 50\n"
     ]
    }
   ],
   "source": [
    "devDF = read_disp_info(\"data/devices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame transformations typically return another DataFrame. Try using a select to return a DataFrame with only the make and model columns, name this dataframe makeModelDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeModelDF = devDF.select('make', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the schema of the makeModelDF, what can you deduce ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "makeModelDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations in a query can be chained together. Write the line of code that will run a query using select the columns devnum, make, and model and where the make is equal to \"Ronin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------+\n",
      "|devnum| make|         model|\n",
      "+------+-----+--------------+\n",
      "|    15|Ronin|Novelty Note 1|\n",
      "|    17|Ronin|Novelty Note 3|\n",
      "|    18|Ronin|Novelty Note 2|\n",
      "|    19|Ronin|Novelty Note 4|\n",
      "|    46|Ronin|            S4|\n",
      "|    47|Ronin|            S1|\n",
      "|    48|Ronin|            S3|\n",
      "|    49|Ronin|            S2|\n",
      "+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "devDF.select(\"devnum\", \"make\", \"model\").where(devDF.make == \"Ronin\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can also read parquet files, parquet is one of the most used format in hdfs, search in the doc and write the line of the code that will let you read the parquet file present in parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"data/base_stations.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_df =  spark.read.parquet(parquet_path)\n",
    "stations_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, select the id, lat and lon columns where the city = Scottsdale from this dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+\n",
      "| id|    lat|      lon|\n",
      "+---+-------+---------+\n",
      "| 14|33.6165|-111.9554|\n",
      "| 15|33.6968|-111.8892|\n",
      "+---+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_df.select('id', 'lat', 'lon').where(stations_df.city == 'Scottsdale').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Definition of a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame based on the devices.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"data/devices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the schema of the devDF DataFrame. Note the column names and types that Spark inferred from the JSON file. In particular, note that the release_dt column is of type string, whereas the data in the column actually represents a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dev_type: string (nullable = true)\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a schema that correctly specifies the column types for this DataFrame starts by importing the package with the definitions of necessary classes and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a collection of StructField objects, which represent column definitions. The release_dt column will be a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "devColumns = [StructField(\"devnum\",LongType()), StructField(\"make\",StringType()), StructField(\"model\",StringType()), StructField(\"release_dt\",TimestampType()),StructField(\"dev_type\",StringType())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a schema (a StructType object) using the column definition list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "devSchema = StructType(devColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have created the devSchema, search in the doc how you can specify the schema when you create the dataframe from the json_file_path (\"/path/to/devices.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = spark.read.schema(devSchema).json(\"data/devices.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the schema and data of the new DataFrame, and confirm that the release_dt column type is now timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- devnum: long (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- release_dt: timestamp (nullable = true)\n",
      " |-- dev_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the line of code that will read the account_device_path correctly, search how to set up the delimiter and consider the header when reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_device_path = \"data/accountdevice/part-00000-f3b62dad-1054-4b2e-81fd-26e54c2ae76a.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+---------------+--------------------+\n",
      "|   id|account_id|device_id|activation_date|   account_device_id|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n",
      "|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n",
      "|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n",
      "|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n",
      "|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n",
      "|48697|     32447|        6|  1342578469000|cc8e8361-3d67-4be...|\n",
      "|48698|     32447|       29|  1386643231000|b40dba90-b073-405...|\n",
      "|48699|     32448|        5|  1350883104000|f088d30f-1e1c-47f...|\n",
      "|48700|     32448|       29|  1383321310000|2805df93-2e89-433...|\n",
      "|48701|     32449|       34|  1333225574000|e0e7edbe-77fc-421...|\n",
      "|48702|     32450|       13|  1341361160000|b3ce579f-48e3-43e...|\n",
      "|48703|     32451|       21|  1329524457000|857bbe4f-028c-475...|\n",
      "|48704|     32451|       29|  1383361574000|826e4946-48a1-43c...|\n",
      "|48705|     32452|       16|  1348532412000|a3c1d199-f70f-44f...|\n",
      "|48706|     32452|       29|  1391579929000|d3e29743-fb26-496...|\n",
      "|48707|     32453|        3|  1327154730000|1c1011ff-9760-410...|\n",
      "|48708|     32454|       14|  1335835131000|253febd2-c2c6-406...|\n",
      "|48709|     32455|       14|  1327343339000|2c4db96f-4afa-411...|\n",
      "|48710|     32455|       29|  1392305172000|96f69b89-adbf-46b...|\n",
      "|48711|     32456|       18|  1351503504000|f7e3fe6a-86bf-4ce...|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(account_device_path).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the function read_disp_info(), and add to it two optional parameters (header and delimiter) those two optional paramaters can be used optionnally and have the default values header = False and delimiter = \";\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_read_disp_info(file, header=False, delimiter=';'):\n",
    "    _, extension = os.path.splitext(file)\n",
    "    print(extension)\n",
    "    if extension == '.csv':\n",
    "        df = spark.read.option(\"delimiter\", delimiter).option(\"header\", header).csv(file)\n",
    "    elif extension == '.json':\n",
    "        df = spark.read.json(file)\n",
    "    else:\n",
    "        raise Exception('Wrong file: csv or json needed')\n",
    "    df.printSchema()\n",
    "    print(df.head(10))\n",
    "    print(f\"\\nNumber of rows {df.count()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the function defined, test it to read the account_device_path file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".csv\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- account_id: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- activation_date: string (nullable = true)\n",
      " |-- account_device_id: string (nullable = true)\n",
      "\n",
      "[Row(id='48692', account_id='32443', device_id='29', activation_date='1393242509000', account_device_id='7351fed1-f344-4cdc-91b7-509ca3e0aee6'), Row(id='48693', account_id='32444', device_id='4', activation_date='1353649861000', account_device_id='6da22278-ff7a-4618-bc93-601207bedff3'), Row(id='48694', account_id='32445', device_id='9', activation_date='1331819465000', account_device_id='cb993b85-6775-4071-8292-3371e173b797'), Row(id='48695', account_id='32446', device_id='43', activation_date='1336860950000', account_device_id='48ea2c09-a0df-4d17-b1da-06a989f4270c'), Row(id='48696', account_id='32446', device_id='29', activation_date='1383650663000', account_device_id='4b49c0a6-d141-42e6-b3d2-767338bfbece'), Row(id='48697', account_id='32447', device_id='6', activation_date='1342578469000', account_device_id='cc8e8361-3d67-4be5-baca-ac885d78fc4e'), Row(id='48698', account_id='32447', device_id='29', activation_date='1386643231000', account_device_id='b40dba90-b073-4052-92cf-e5fe2b216319'), Row(id='48699', account_id='32448', device_id='5', activation_date='1350883104000', account_device_id='f088d30f-1e1c-47f4-b4ad-d73eb7e760e5'), Row(id='48700', account_id='32448', device_id='29', activation_date='1383321310000', account_device_id='2805df93-2e89-433c-a211-b5d6cd95a685'), Row(id='48701', account_id='32449', device_id='34', activation_date='1333225574000', account_device_id='e0e7edbe-77fc-421e-8806-28e2d5a755ed')]\n",
      "\n",
      "Number of rows 194764\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|   id|account_id|device_id|activation_date|   account_device_id|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "|48692|     32443|       29|  1393242509000|7351fed1-f344-4cd...|\n",
      "|48693|     32444|        4|  1353649861000|6da22278-ff7a-461...|\n",
      "|48694|     32445|        9|  1331819465000|cb993b85-6775-407...|\n",
      "|48695|     32446|       43|  1336860950000|48ea2c09-a0df-4d1...|\n",
      "|48696|     32446|       29|  1383650663000|4b49c0a6-d141-42e...|\n",
      "|48697|     32447|        6|  1342578469000|cc8e8361-3d67-4be...|\n",
      "|48698|     32447|       29|  1386643231000|b40dba90-b073-405...|\n",
      "|48699|     32448|        5|  1350883104000|f088d30f-1e1c-47f...|\n",
      "|48700|     32448|       29|  1383321310000|2805df93-2e89-433...|\n",
      "|48701|     32449|       34|  1333225574000|e0e7edbe-77fc-421...|\n",
      "|48702|     32450|       13|  1341361160000|b3ce579f-48e3-43e...|\n",
      "|48703|     32451|       21|  1329524457000|857bbe4f-028c-475...|\n",
      "|48704|     32451|       29|  1383361574000|826e4946-48a1-43c...|\n",
      "|48705|     32452|       16|  1348532412000|a3c1d199-f70f-44f...|\n",
      "|48706|     32452|       29|  1391579929000|d3e29743-fb26-496...|\n",
      "|48707|     32453|        3|  1327154730000|1c1011ff-9760-410...|\n",
      "|48708|     32454|       14|  1335835131000|253febd2-c2c6-406...|\n",
      "|48709|     32455|       14|  1327343339000|2c4db96f-4afa-411...|\n",
      "|48710|     32455|       29|  1392305172000|96f69b89-adbf-46b...|\n",
      "|48711|     32456|       18|  1351503504000|f7e3fe6a-86bf-4ce...|\n",
      "+-----+----------+---------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_read_disp_info(account_device_path, True, \",\").show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d31793bc0164bbf1a7493650a68c8e00179ba062bd5a26b747d14cdd740a9f47"
  },
  "kernelspec": {
   "display_name": "Python 3.7 - Spark (local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
